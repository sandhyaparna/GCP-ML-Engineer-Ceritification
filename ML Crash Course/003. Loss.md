* Model starts with random weights --> loss is computed & gradient of loss helps in --> parameter updation --> again calculate loss --> repeat till loss no longer changes/decreases --> implies model has converged
* very low learning rate requires a lot of epochs for conversion
* Loss can be decreased by efficiently choosing the direction to update parameters
* Gradient is a vector with magnitude & direction
* If learning rate is large, take big steps (step size = learning rate * gradient)
* If learning rate is small, takes small step size
* In case of non-convex optimization problems, where there are many local minima, weights initiation matters
* Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called **empirical risk minimization**
* The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. 
* A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss.
* The model learning continues iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has converged.

### Optimization algorithms
* Stochastic Gradient Descent: 1 example at a time
* Mini-batch Gradient Descent: Weights are updates a fter a batch of examples







